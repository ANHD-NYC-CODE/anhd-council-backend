version: "2"
services:
  app:
    restart: always
    build: .
    expose:
      - "8000"
    container_name: "app"
    # image: debian/latest
    image: app_image
    links:
      - postgres
      - redis
    depends_on:
      - postgres
      - redis
      - celerybeat
    ports:
      - "8000:8000"
    networks:
      - network1
      - nginx_network
    volumes:
      - ./:/app
      - ./data:/app/data
      - static_volume:/app/static
      - ./logs:/app/logs
    entrypoint: ["sh", "/app/docker-entrypoint.sh"]
    env_file:
      - .env
    environment:
      - DJANGO_SETTINGS_MODULE=app.settings.production
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  celery_default:
    restart: always
    build: .
    container_name: "celery_default"
    image: app_image
    networks:
      - network1
    links:
      - redis
      - postgres
    depends_on:
      - postgres
      - redis
      - celerybeat
    volumes:
      - ./:/app
      - ./data:/app/data
      - ./logs:/app/logs
      - ./celery:/app/celery
    env_file:
      - .env
    entrypoint: "celery -A app worker -E -Q celery -l debug -n celery_worker --concurrency=8 --logfile=./celery/logs/default.log"
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  celery_update:
    restart: always
    build: .
    container_name: "celery_update"
    image: app_image
    networks:
      - network1
    depends_on:
      - postgres
      - redis
      - celerybeat
    volumes:
      - ./:/app
      - ./data:/app/data
      - ./logs:/app/logs
      - ./celery/logs:/app/celery/logs
    links:
      - postgres
      - redis
    env_file:
      - .env
    entrypoint: "celery -A app worker -E -Q update -l debug -n update_worker --concurrency=8 --logfile=./celery/logs/update.log"
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  celerybeat:
    restart: always
    build: .
    container_name: "celerybeat"
    image: app_image
    entrypoint: "celery -A app beat -l debug --scheduler django_celery_beat.schedulers:DatabaseScheduler --logfile=./celery/logs/beat.log"
    networks:
      - network1
    depends_on:
      - postgres
      - redis
    links:
      - postgres
      - redis
    volumes:
      - ./:/app
      - ./celery:/app/celery
    env_file:
      - .env
    # https://www.distributedpython.com/2018/10/13/flower-docker/
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  flower:
    image: mher/flower
    container_name: "flower"
    expose:
      - "8888"
    environment:
      - CELERY_BROKER_URL=redis://redis
      - FLOWER_PORT=8888
    networks:
      - network1
      - nginx_network
    depends_on:
      - redis
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  redis:
    restart: always
    hostname: redis
    container_name: "redis"
    image: redis:alpine
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - redis_data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    networks:
      - network1
    env_file:
      - .env
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  postgres:
    image: postgres:11
    container_name: "postgres"
    restart: on-failure
    # https://blog.2ndquadrant.com/basics-of-tuning-checkpoints/
    # linux OOM overcommit https://www.postgresql.org/docs/current/kernel-resources.html#LINUX-MEMORY-OVERCOMMIT
    command:
      [
        "-c",
        "autovacuum_vacuum_scale_factor=0.05",
        "-c",
        "vacuum_cost_page_hit=1",
        "-c",
        "vacuum_cost_page_miss=10",
        "-c",
        "vacuum_cost_page_dirty=20",
        "-c",
        "autovacuum_vacuum_cost_delay=20ms",
        "-c",
        "autovacuum_vacuum_cost_limit=1000",
        "-c",
        "enable_seqscan=ON",
        "-c",
        "huge_pages=try",
        "-c",
        "effective_io_concurrency=1",
        "-c",
        "max_parallel_workers_per_gather=4",
        "-c",
        "dynamic_shared_memory_type=posix",
        "-c",
        "max_stack_depth=6MB",
        "-c",
        "maintenance_work_mem=1GB",
        "-c",
        "temp_buffers=64MB",
        "-c",
        "shared_buffers=4GB",
        "-c",
        "default_statistics_target=1000",
        "-c",
        "checkpoint_timeout=120min",
        "-c",
        "checkpoint_completion_target=0.93",
        "-c",
        "max_wal_size=6GB",
        "-c",
        "effective_cache_size=8GB",
        "-c",
        "work_mem=600MB",
        "-c",
        "max_connections=200",
        "-c",
        "random_page_cost=1.05",
        "-c",
        "seq_page_cost=2",
        "-c",
        "log_min_duration_statement=10000",
        "-c",
        "log_temp_files=1",
      ]
    shm_size: "2GB"
    environment:
      - POSTGRES_DB=anhd
      - POSTGRES_USER=anhd
    networks:
      - network1
    volumes:
      - pg_vol1:/var/lib/postgresql/data
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  nginx:
    build: ./nginx
    container_name: "nginx_server"
    # https://medium.com/@pentacent/nginx-and-lets-encrypt-with-docker-in-less-than-5-minutes-b4b8a60d3a71
    logging:
      options:
        max-size: "10m"
        max-file: "3"
  certbot:
    container_name: "certbot"
    image: certbot/certbot
    volumes:
      - ./nginx/certbot/conf:/etc/letsencrypt
      - ./nginx/certbot/www:/var/www/certbot
    depends_on:
      - nginx
    logging:
      options:
        max-size: "10m"
        max-file: "3"
volumes:
  pg_vol1:
  redis_data:
  static_volume:
networks:
  network1:
  nginx_network:
    driver: bridge
